{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, AutoModelForCausalLM, GPT2DoubleHeadsModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.add_special_tokens({'mask_token': '[MASK]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(54473, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "with open('../data/finalWords.json') as f:\n",
    "    words = json.load(f)\n",
    "\n",
    "model_name = 'gpt2'\n",
    "\n",
    "# Add the new words to the tokenizer\n",
    "new_word_list = [str(word_obj[\"word\"]) for word_obj in words]\n",
    "new_def_list = [str(word_obj[\"definition\"]) for word_obj in words]\n",
    "\n",
    "# Check if each new word is already in the GPT2 vocabulary\n",
    "words_to_add = []\n",
    "vocab = tokenizer.get_vocab()\n",
    "for word in new_word_list:\n",
    "    if word not in vocab:\n",
    "        words_to_add.append(word)\n",
    "        \n",
    "for definition in new_def_list:\n",
    "    for word in definition.split():\n",
    "        if word not in vocab:\n",
    "            words_to_add.append(word)\n",
    "\n",
    "num_new_words = len(new_word_list)\n",
    "num_added = tokenizer.add_tokens(words_to_add)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749\n",
      "('Janky', 'Undesirable; less-than optimum.', 'brutal', 'anything that makes you sweat', \"Man, this morning's calisthenics were [MASK].\", tensor(50261))\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "for i, x in enumerate(words):\n",
    "    word1 = x['word']\n",
    "    definition_str1 = x['definition']\n",
    "    word1_onehot = tokenizer.encode(word1, add_special_tokens=False, return_tensors='pt').squeeze()\n",
    "    # Get second word and definition\n",
    "    j = i + 1\n",
    "    if j == len(words):\n",
    "        break\n",
    "\n",
    "    x = words[j]\n",
    "\n",
    "    word2 = x['word']\n",
    "    definition_str2 = x['definition']\n",
    "    word2_onehot = tokenizer.encode(word2, add_special_tokens=False, return_tensors='pt').squeeze()\n",
    "    # Get example sentence and mask out target word\n",
    "    example = x['example']\n",
    "\n",
    "    check = -1\n",
    "    masked_example = ''\n",
    "    if word1 in example:\n",
    "        masked_example = example.replace(word1, '[MASK]')\n",
    "        training_data.append((word1, definition_str1, word2, definition_str2, masked_example, word1_onehot)) #recheck\n",
    "    elif word2 in example:\n",
    "        masked_example = example.replace(word2, '[MASK]')\n",
    "        training_data.append((word1, definition_str1, word2, definition_str2, masked_example, word2_onehot)) #recheck\n",
    "print(len(training_data))\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(data):\n",
    "    word1 = [x[0] for x in data]\n",
    "    sent1 = [x[1] for x in data]\n",
    "    word2 = [x[2] for x in data]\n",
    "    sent2 = [x[3] for x in data]\n",
    "    example = [x[4] for x in data]\n",
    "    labels = [x[5] for x in data]\n",
    "\n",
    "    if any(isinstance(l, list) for l in labels):\n",
    "        labels = [item for sublist in labels for item in sublist]\n",
    "\n",
    "    combined_text = []\n",
    "    for w1, s1, w2, s2, ex in zip(word1, sent1, word2, sent2, example):\n",
    "        combined_text.append(w1 + \" \" + s1 + \" \" + w2 + \" \" + s2 + \" \" + ex)\n",
    "        \n",
    "    encoding = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    input_ids = torch.LongTensor(encoding['input_ids'])\n",
    "    attention_mask = torch.LongTensor(encoding['attention_mask'])\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return (input_ids, attention_mask, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 11/150 [00:08<01:43,  1.35it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filtered_outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m     37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(filtered_outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokenizer)), labels\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the training parameters\n",
    "batch_size = 5\n",
    "num_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune the model on the training data\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    max_len = 0\n",
    "    for i in tqdm(range(0, len(training_data), batch_size)):\n",
    "        batch_data = training_data[i:i+batch_size]\n",
    "        input_ids, attention_mask, labels = pad_data(batch_data)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Creates a binary mask indicating where the [MASK] token is located in the input IDs, \n",
    "        #then shifts it one position to the left to align with the target labels.\n",
    "        temp_mask = input_ids == torch.tensor(tokenizer.convert_tokens_to_ids('[MASK]'))      \n",
    "        temp_mask = temp_mask[:, 1:].squeeze(-1)\n",
    "\n",
    "        #Runs the input IDs through the model to get the output logits, then slices off the last token to match the length of the target labels.\n",
    "        outputs = model(input_ids, attention_mask=attention_mask).logits[:, :-1, :]\n",
    "\n",
    "        #Applies the binary mask to the output logits to get the values corresponding to the [MASK] tokens.\n",
    "        filtered_outputs = outputs[temp_mask] \n",
    "\n",
    "        #if filtered_outputs.shape[0] == 8:\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "        #filter bad batches, perform backpropagation, and update the model parameters\n",
    "        if filtered_outputs.shape[0] == 5:\n",
    "            loss = criterion(filtered_outputs.reshape(-1, len(tokenizer)), labels.reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    print(\"Epoch {} loss: {:.4f}\".format(epoch+1, epoch_loss/num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2DoubleHeadsModel were not initialized from the model checkpoint at ../models/paradigm-model and are newly initialized: ['multiple_choice_head.summary.bias', 'multiple_choice_head.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel\n",
    "\n",
    "# Assuming your fine-tuned model is stored in a variable called `model`\n",
    "model.save_pretrained(\"../models/paradigm-model\")\n",
    "\n",
    "# Assuming your fine-tuned tokenizer is stored in a variable called `tokenizer`\n",
    "tokenizer.save_pretrained(\"../models/paradigm-token\")\n",
    "\n",
    "model_path = \"../models/paradigm-model\"\n",
    "tokenizer_path = \"../models/paradigm-token\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "model = GPT2DoubleHeadsModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brutal anything that makes you sweat That skanky ho-bag wants to borrow your homework. [CLS]', 'skanky Anything of or pertaining to a $10,000 hooker. That skanky ho-bag wants to borrow your homework. [CLS]']\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "for i, x in enumerate(words):\n",
    "    options = []\n",
    "    word1 = x['word']\n",
    "    definition_str1 = x['definition']\n",
    "    # Get second word and definition\n",
    "    j = i + 1\n",
    "    if j == len(words):\n",
    "        break\n",
    "\n",
    "    x = words[j]\n",
    "\n",
    "    word2 = x['word']\n",
    "    definition_str2 = x['definition']\n",
    "    word2_onehot = tokenizer.encode(word2, add_special_tokens=False, return_tensors='pt').squeeze()\n",
    "    # Get example sentence and mask out target word\n",
    "    example = x['example']\n",
    "\n",
    "    check = -1\n",
    "    masked_example = ''\n",
    "    masked_example = example.replace(word1, word2)\n",
    "    options.append(word1 + \" \" + definition_str1 + \" \" + masked_example + \" [CLS]\")\n",
    "    options.append(word2 + \" \" + definition_str2 + \" \" + example + \" [CLS]\") \n",
    "    questions.append(options)\n",
    "    answers.append(1)\n",
    "print(questions[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a [CLS] to the vocabulary (we should train it also!)\n",
    "num_added_tokens = tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})\n",
    "# Update the model embeddings with the new vocabulary size\n",
    "embedding_layer = model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count0 = 0.0\n",
    "total = 0.0\n",
    "for i, choices in enumerate(questions):\n",
    "    encoded_choices = [tokenizer.encode(s) for s in choices]\n",
    "    cls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]\n",
    "    #check if some choices are longer than the others\n",
    "    max_len = max([len(tokens) for tokens in encoded_choices])\n",
    "    #print(max_len)\n",
    "    check = True\n",
    "\n",
    "    for tokens in encoded_choices:\n",
    "        if len(tokens) < max_len:\n",
    "            check = False\n",
    "    if check:\n",
    "        input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\n",
    "        mc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1\n",
    "        outputs = model(input_ids, mc_token_ids=mc_token_ids)\n",
    "        lm_logits = outputs.logits\n",
    "        mc_logits = outputs.mc_logits\n",
    "        if mc_logits.argmax().item() == 0:\n",
    "            count0 += 1\n",
    "        \n",
    "        total += 1\n",
    "print(count0/total)\n",
    "\n",
    "print(len(questions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c259f8a0ab8cae8600c2d914888b9c4f54382130559791b634cf67ec39ea37f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
