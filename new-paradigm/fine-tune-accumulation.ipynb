{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, AutoModelForCausalLM, GPT2DoubleHeadsModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.add_special_tokens({'mask_token': '[MASK]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(54473, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "with open('../data/finalWords.json') as f:\n",
    "    words = json.load(f)\n",
    "\n",
    "model_name = 'gpt2'\n",
    "\n",
    "# Add the new words to the tokenizer\n",
    "new_word_list = [str(word_obj[\"word\"]) for word_obj in words]\n",
    "new_def_list = [str(word_obj[\"definition\"]) for word_obj in words]\n",
    "\n",
    "# Check if each new word is already in the GPT2 vocabulary\n",
    "words_to_add = []\n",
    "vocab = tokenizer.get_vocab()\n",
    "for word in new_word_list:\n",
    "    if word not in vocab:\n",
    "        words_to_add.append(word)\n",
    "        \n",
    "for definition in new_def_list:\n",
    "    for word in definition.split():\n",
    "        if word not in vocab:\n",
    "            words_to_add.append(word)\n",
    "\n",
    "num_new_words = len(new_word_list)\n",
    "num_added = tokenizer.add_tokens(words_to_add)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749\n",
      "('Janky', 'Undesirable; less-than optimum.', 'brutal', 'anything that makes you sweat', \"Man, this morning's calisthenics were [MASK].\", tensor(50261))\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "for i, x in enumerate(words):\n",
    "    word1 = x['word']\n",
    "    definition_str1 = x['definition']\n",
    "    word1_onehot = tokenizer.encode(word1, add_special_tokens=False, return_tensors='pt').squeeze()\n",
    "    # Get second word and definition\n",
    "    j = i + 1\n",
    "    if j == len(words):\n",
    "        break\n",
    "\n",
    "    x = words[j]\n",
    "\n",
    "    word2 = x['word']\n",
    "    definition_str2 = x['definition']\n",
    "    word2_onehot = tokenizer.encode(word2, add_special_tokens=False, return_tensors='pt').squeeze()\n",
    "    # Get example sentence and mask out target word\n",
    "    example = x['example']\n",
    "\n",
    "    check = -1\n",
    "    masked_example = ''\n",
    "    if word1 in example:\n",
    "        masked_example = example.replace(word1, '[MASK]')\n",
    "        training_data.append((word1, definition_str1, word2, definition_str2, masked_example, word1_onehot)) #recheck\n",
    "    elif word2 in example:\n",
    "        masked_example = example.replace(word2, '[MASK]')\n",
    "        training_data.append((word1, definition_str1, word2, definition_str2, masked_example, word2_onehot)) #recheck\n",
    "print(len(training_data))\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(data):\n",
    "    word1 = [x[0] for x in data]\n",
    "    sent1 = [x[1] for x in data]\n",
    "    word2 = [x[2] for x in data]\n",
    "    sent2 = [x[3] for x in data]\n",
    "    example = [x[4] for x in data]\n",
    "    labels = [x[5] for x in data]\n",
    "\n",
    "    if any(isinstance(l, list) for l in labels):\n",
    "        labels = [item for sublist in labels for item in sublist]\n",
    "\n",
    "    combined_text = []\n",
    "    for w1, s1, w2, s2, ex in zip(word1, sent1, word2, sent2, example):\n",
    "        combined_text.append(w1 + \" \" + s1 + \" \" + w2 + \" \" + s2 + \" \" + ex)\n",
    "        \n",
    "    encoding = tokenizer(combined_text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    input_ids = torch.LongTensor(encoding['input_ids'])\n",
    "    attention_mask = torch.LongTensor(encoding['attention_mask'])\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return (input_ids, attention_mask, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:07<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 6.7382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 5.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:05<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 4.3942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 4.1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:06<00:00,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 4.2348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the training parameters\n",
    "batch_size = 5\n",
    "num_epochs = 5\n",
    "learning_rate = 5e-5\n",
    "accumulation_steps = 2  # accumulate gradients over 2 batches\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune the model on the training data\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    max_len = 0\n",
    "    accumulation_steps_count = 0 # initialize accumulation steps count\n",
    "    for i in tqdm(range(0, len(training_data[:3000]), batch_size)):\n",
    "        batch_data = training_data[i:i+batch_size]\n",
    "        input_ids, attention_mask, labels = pad_data(batch_data)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Creates a binary mask indicating where the [MASK] token is located in the input IDs, \n",
    "        # then shifts it one position to the left to align with the target labels.\n",
    "        temp_mask = input_ids == torch.tensor(tokenizer.convert_tokens_to_ids('[MASK]'))      \n",
    "        temp_mask = temp_mask[:, 1:].squeeze(-1)\n",
    "\n",
    "        # Runs the input IDs through the model to get the output logits, then slices off the last token to match the length of the target labels.\n",
    "        outputs = model(input_ids, attention_mask=attention_mask).logits[:, :-1, :]\n",
    "\n",
    "        # Applies the binary mask to the output logits to get the values corresponding to the [MASK] tokens.\n",
    "        filtered_outputs = outputs[temp_mask] \n",
    "\n",
    "        # Filter bad batches and calculate loss\n",
    "        if filtered_outputs.shape[0] == batch_size:\n",
    "            loss = criterion(filtered_outputs.reshape(-1, len(tokenizer)), labels.reshape(-1))\n",
    "\n",
    "            # Calculate gradients\n",
    "            loss = loss / accumulation_steps  # divide loss by accumulation steps\n",
    "            loss.backward()\n",
    "\n",
    "            # Accumulate gradients over multiple batches\n",
    "            accumulation_steps_count += 1\n",
    "            if accumulation_steps_count == accumulation_steps:\n",
    "                optimizer.step()  # update model parameters\n",
    "                optimizer.zero_grad()  # reset gradients\n",
    "                accumulation_steps_count = 0  # reset accumulation steps count\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    print(\"Epoch {} loss: {:.4f}\".format(epoch+1, epoch_loss/num_batches))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2DoubleHeadsModel were not initialized from the model checkpoint at ../models/paradigm-model and are newly initialized: ['multiple_choice_head.summary.bias', 'multiple_choice_head.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel\n",
    "\n",
    "# Assuming your fine-tuned model is stored in a variable called `model`\n",
    "model.save_pretrained(\"../models/paradigm-model\")\n",
    "\n",
    "# Assuming your fine-tuned tokenizer is stored in a variable called `tokenizer`\n",
    "tokenizer.save_pretrained(\"../models/paraidgm-token\")\n",
    "\n",
    "model_path = \"../models/paradigm-model\"\n",
    "tokenizer_path = \"../models/paraidgm-token\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "model = GPT2DoubleHeadsModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brutal anything that makes you sweat That skanky ho-bag wants to borrow your homework. [CLS]', 'skanky Anything of or pertaining to a $10,000 hooker. That skanky ho-bag wants to borrow your homework. [CLS]']\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "for i, x in enumerate(words):\n",
    "    options = []\n",
    "    word1 = x['word']\n",
    "    definition_str1 = x['definition']\n",
    "    # Get second word and definition\n",
    "    j = i + 1\n",
    "    if j == len(words):\n",
    "        break\n",
    "\n",
    "    x = words[j]\n",
    "\n",
    "    word2 = x['word']\n",
    "    definition_str2 = x['definition']\n",
    "    word2_onehot = tokenizer.encode(word2, add_special_tokens=False, return_tensors='pt').squeeze()\n",
    "    # Get example sentence and mask out target word\n",
    "    example = x['example']\n",
    "\n",
    "    check = -1\n",
    "    masked_example = ''\n",
    "    masked_example = example.replace(word2, word1)\n",
    "    options.append(word1 + \" \" + definition_str1 + \" \" + word2 + \" \" + definition_str2 + \" \" + masked_example + \" [CLS]\")\n",
    "    options.append(word1 + \" \" + definition_str1 + \" \" + word2 + \" \" + definition_str2 + \" \" + example + \" [CLS]\") \n",
    "    questions.append(options)\n",
    "    answers.append(1)\n",
    "print(questions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a [CLS] to the vocabulary (we should train it also!)\n",
    "num_added_tokens = tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})\n",
    "# Update the model embeddings with the new vocabulary size\n",
    "embedding_layer = model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43333333333333335\n",
      "1070\n"
     ]
    }
   ],
   "source": [
    "count0 = 0.0\n",
    "total = 0.0\n",
    "for i, choices in enumerate(questions):\n",
    "    #print(choices)\n",
    "    #print(choices)\n",
    "    \n",
    "    #print(choices)\n",
    "    encoded_choices = [tokenizer.encode(s) for s in choices]\n",
    "    cls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]\n",
    "    #check if some choices are longer than the others\n",
    "    max_len = max([len(tokens) for tokens in encoded_choices])\n",
    "    #print(max_len)\n",
    "    check = True\n",
    "    \n",
    "    #print(len(encoded_choices))\n",
    "    #print(encoded_choices[0])\n",
    "    #print(encoded_choices[1])\n",
    "    for tokens in encoded_choices:\n",
    "        if len(tokens) < max_len:\n",
    "            check = False\n",
    "    if check:\n",
    "        input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\n",
    "        mc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1\n",
    "        outputs = model(input_ids, mc_token_ids=mc_token_ids)\n",
    "        lm_logits = outputs.logits\n",
    "        mc_logits = outputs.mc_logits\n",
    "        if mc_logits.argmax().item() == 0:\n",
    "            count0 += 1\n",
    "        \n",
    "        total += 1\n",
    "print(count0/total)\n",
    "\n",
    "print(len(questions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c259f8a0ab8cae8600c2d914888b9c4f54382130559791b634cf67ec39ea37f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
