{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b132ff9688f34d1c9aa42c7bc44d6ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4551a530fe88452481a16c3e452c13f5",
              "IPY_MODEL_b0079765d26f4e2c8f395308d180df0c",
              "IPY_MODEL_39430bfdd688405ab88760258ef170f1"
            ],
            "layout": "IPY_MODEL_76a99c286588471b84643a5a4e98361d"
          }
        },
        "4551a530fe88452481a16c3e452c13f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b142aa906ab54187b7eaa0959da0f0da",
            "placeholder": "​",
            "style": "IPY_MODEL_d85130dbaf85446899402f6b069d4886",
            "value": "100%"
          }
        },
        "b0079765d26f4e2c8f395308d180df0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab91a713275045dc9b0f9729d5bde313",
            "max": 15430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af137512a4d44c7a958b85364f1aaa9d",
            "value": 15430
          }
        },
        "39430bfdd688405ab88760258ef170f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5347dae6bea4798aca391ff0777f306",
            "placeholder": "​",
            "style": "IPY_MODEL_ed1419332507467eb4ff994ab35b90fe",
            "value": " 15430/15430 [3:22:41&lt;00:00,  1.49it/s]"
          }
        },
        "76a99c286588471b84643a5a4e98361d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b142aa906ab54187b7eaa0959da0f0da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d85130dbaf85446899402f6b069d4886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab91a713275045dc9b0f9729d5bde313": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af137512a4d44c7a958b85364f1aaa9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5347dae6bea4798aca391ff0777f306": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed1419332507467eb4ff994ab35b90fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee302dbe362e4b4abc87d86d2205c9b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_380ea7e671b1401596bbb76decfd2e12",
              "IPY_MODEL_7a019006eb164a7da9b7023997f7dede",
              "IPY_MODEL_f1ced2c4ceea4d5fa6b6ea6037e67c8d"
            ],
            "layout": "IPY_MODEL_6bb9af573f3347498e64996002b9993d"
          }
        },
        "380ea7e671b1401596bbb76decfd2e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ef1a1c74d1d48feb5a9e31e44d1ac0a",
            "placeholder": "​",
            "style": "IPY_MODEL_a5dbe34bcab24e33a7b130164a48f893",
            "value": "100%"
          }
        },
        "7a019006eb164a7da9b7023997f7dede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38942c0c9b3040a5bb189b592646fcd6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5cfd6f4efaaf42d48b62eb4652ce2da1",
            "value": 2
          }
        },
        "f1ced2c4ceea4d5fa6b6ea6037e67c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a378b91c1254a06b68d05ad80f6d50a",
            "placeholder": "​",
            "style": "IPY_MODEL_48e3fba0d646477ebc7615b608ec7782",
            "value": " 2/2 [00:00&lt;00:00, 83.41it/s]"
          }
        },
        "6bb9af573f3347498e64996002b9993d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ef1a1c74d1d48feb5a9e31e44d1ac0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5dbe34bcab24e33a7b130164a48f893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38942c0c9b3040a5bb189b592646fcd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cfd6f4efaaf42d48b62eb4652ce2da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a378b91c1254a06b68d05ad80f6d50a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48e3fba0d646477ebc7615b608ec7782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDEPXsyw07zr",
        "outputId": "6c9cf623-c79a-4ad0-fad2-77c780ea527a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.9/dist-packages (0.4.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.13.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.10.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWnLeZJvjUXI",
        "outputId": "978c8510-e80e-490c-b86d-f389e5e27c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/data/\n",
        "# %ls\n",
        "# %mkdir gpt2_casual"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQgL-W9vjZth",
        "outputId": "f6d4c652-3c09-4778-f848-7d4c3784057e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfbUc9WhjaaR",
        "outputId": "ae54d3a7-8779-496b-d836-faebcbd687a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, GPT2DoubleHeadsModel\n",
        "import torch\n",
        "import json\n",
        "\n",
        "\n",
        "# Load the JSON file\n",
        "with open('finalWords.json') as f:\n",
        "    words = json.load(f)\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "model = GPT2DoubleHeadsModel.from_pretrained(\"gpt2\")\n",
        "model = model.to(device)\n",
        "\n",
        "original_vocab_size = len(tokenizer)\n",
        "new_word_list = [i.strip().lower() for i in open(\"words.txt\").readlines()]\n",
        "num_new_words = len(new_word_list)\n",
        "\n",
        "num_added = tokenizer.add_tokens(new_word_list)\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# new_word_list = [i.strip().lower() for i in open(\"words.txt\").readlines()]\n",
        "# num_new_words = len(new_word_list)\n",
        "# num_added = tokenizer.add_tokens(new_word_list)\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# #Compute the distribution from which we’ll sample:\n",
        "# params = model.state_dict()\n",
        "# embeddings = params['transformer.wte.weight']\n",
        "# pre_expansion_embeddings = embeddings[:-num_new_words,:]\n",
        "# mu = torch.mean(pre_expansion_embeddings, dim=0)\n",
        "# n = pre_expansion_embeddings.size()[0]\n",
        "# sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n\n",
        "# dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
        "#         mu, covariance_matrix=1e-5*sigma)\n",
        "\n",
        "# #load in our new embeddings into the model:\n",
        "# new_embeddings = torch.stack(tuple((dist.sample() for _ in range(num_new_words))), dim=0)\n",
        "# embeddings[-num_new_words:,:] = new_embeddings\n",
        "# params['transformer.wte.weight'][-num_new_words:,:] = new_embeddings\n",
        "# model.load_state_dict(params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvyBmJ5I1EUv",
        "outputId": "dda99e17-2220-49c7-ff4f-af90623627a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2DoubleHeadsModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['multiple_choice_head.summary.weight', 'multiple_choice_head.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(100258, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "text = load_dataset(\"text\",data_files={\"train\": \"finetune_train.txt\", \"test\": \"finetune_val.txt\"})\n",
        "# print(type(text[\"train\"]))\n",
        "# text[\"train\"] = text[\"train\"].train_test_split(test_size=0.02)[\"test\"]\n",
        "# text[\"test\"] = text[\"test\"].train_test_split(test_size=0.02)[\"test\"]\n",
        "# text[\"test\"] = text[\"test\"][:\n",
        "# print(text['train'])\n",
        "# text = text['train'].train_test_split(test_size=0.2)\n",
        "print(len(text[\"train\"]), len(text[\"test\"]))\n",
        "# print(text[\"train\"][0:10])\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer([x for x in examples[\"text\"]], truncation=True)\n",
        "tokenized_txt = text.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns=text[\"train\"].column_names\n",
        ")\n",
        "# print(tokenized_txt[\"train \"][0:100])\n",
        "block_size = 128\n",
        "\n",
        "# print(tokenizer(\"My wife had a PMS Strike last night. PMS strikes are ruining my marriage!\"))\n",
        "def group_texts(examples):\n",
        "    # print(examples)\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "lm_dataset = tokenized_txt.map(group_texts, batched=True, num_proc=4)\n"
      ],
      "metadata": {
        "id": "Ti5hbH3g1qKh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "ee302dbe362e4b4abc87d86d2205c9b8",
            "380ea7e671b1401596bbb76decfd2e12",
            "7a019006eb164a7da9b7023997f7dede",
            "f1ced2c4ceea4d5fa6b6ea6037e67c8d",
            "6bb9af573f3347498e64996002b9993d",
            "4ef1a1c74d1d48feb5a9e31e44d1ac0a",
            "a5dbe34bcab24e33a7b130164a48f893",
            "38942c0c9b3040a5bb189b592646fcd6",
            "5cfd6f4efaaf42d48b62eb4652ce2da1",
            "5a378b91c1254a06b68d05ad80f6d50a",
            "48e3fba0d646477ebc7615b608ec7782"
          ]
        },
        "outputId": "66eaffab-675f-4c21-c159-07beea30d124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-44d754cc7e7fe124/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee302dbe362e4b4abc87d86d2205c9b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-44d754cc7e7fe124/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-add71312ab63cc0a_*_of_00004.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-44d754cc7e7fe124/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-089bf743f92183db_*_of_00004.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-44d754cc7e7fe124/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-32cb954df5e3beec_*_of_00004.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-44d754cc7e7fe124/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-4dee680c64e12078_*_of_00004.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75001 12500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.mask_token = u\"\\u2047\"\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "WQKvJqXnkJfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import evaluate\n",
        "# model = AutoModelForMaskedLM.from_pretrained(\"distilroberta-base\")\n",
        "# model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "# model = model.to(device)\n",
        "# metric = evaluate.load(\"perplexity\")\n",
        "# def compute_metrics(eval_pred):\n",
        "#     print(eval_pred)\n",
        "#     predictions, labels = eval_pred\n",
        "#     predictions = predictions[:, 0]\n",
        "#     return metric.compute(predictions=predictions, references=labels, model_id='gpt2')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2_finetune_random\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    save_steps = 10000,\n",
        "    num_train_epochs=8,\n",
        "    weight_decay=0.1,\n",
        "    # save_total_limit=1,\n",
        "    # metric_for_best_model='eval_loss',\n",
        "    save_strategy=\"epoch\",\n",
        "    # logging_strategy=\"epoch\",\n",
        "    eval_accumulation_steps = 1\n",
        "    # load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_dataset[\"train\"],\n",
        "    eval_dataset=lm_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "# print(trainer.evaluate(lm_dataset[\"test\"]))\n",
        "trainer.save_model(\"gpt2_finetune_random\")\n",
        "tokenizer.save_pretrained(\"gpt2_finetune_random/tokenizer.json\")"
      ],
      "metadata": {
        "id": "NF9vtpnt1vt8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "8630c2de-1d1a-4f3d-a06f-56786f6e55cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24688' max='24688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [24688/24688 2:56:32, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.357400</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.069100</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.908600</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.820100</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.740500</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.677000</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.628500</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.617900</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt2_finetune_random/tokenizer.json/tokenizer_config.json',\n",
              " 'gpt2_finetune_random/tokenizer.json/special_tokens_map.json',\n",
              " 'gpt2_finetune_random/tokenizer.json/vocab.json',\n",
              " 'gpt2_finetune_random/tokenizer.json/merges.txt',\n",
              " 'gpt2_finetune_random/tokenizer.json/added_tokens.json',\n",
              " 'gpt2_finetune_random/tokenizer.json/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/ayoolaolafenwa/TrainNLP\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from torch.optim import AdamW\n",
        "from torch import optim\n",
        "from transformers import get_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(lm_dataset[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator,)\n",
        "\n",
        "# load the test dataset for evaluation\n",
        "eval_dataloader = DataLoader(lm_dataset[\"test\"], batch_size=batch_size, collate_fn=data_collator)\n",
        "eval_dataset = lm_dataset[\"test\"]\n",
        "# initialize pretrained bert model\n",
        "# model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "# set the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "# initialize accelerator for training\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
        "\n",
        "# set the number of epochs which is set to 30\n",
        "num_train_epochs = 10\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "# define the learning rate scheduler for training\n",
        "lr_scheduler = get_scheduler(\"linear\",optimizer=optimizer,num_warmup_steps=0,num_training_steps=num_training_steps)\n",
        "\n"
      ],
      "metadata": {
        "id": "r3_8oorA0lqV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "17c7559b-0527-4761-c346-4f851f46da4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4120833f58e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#https://github.com/ayoolaolafenwa/TrainNLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'accelerate'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGWuHc-_O4dP",
        "outputId": "4b953090-95fc-4844-bc5d-3c7a44d0497c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.17.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.13.1+cu116)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (4.5.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model = GPT2DoubleHeadsModel.from_pretrained(\"gpt2\")\n",
        "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "checkpoint = torch.load(\"gpt2_casual/2\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']"
      ],
      "metadata": {
        "id": "oqhFWXO5CJ27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_-YDbNF05N3",
        "outputId": "2ab441a6-f1a8-49f4-fd7e-ab86af402ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.17.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 KB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.13.1+cu116)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (4.5.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "# directory to save the models\n",
        "output_dir = \"gpt2_casual/\"\n",
        "val_loss = 1000000\n",
        "num_train_epochs = 10\n",
        "for epoch in range(num_train_epochs):\n",
        "    # torch.save({'epoch': epoch,\n",
        "    #         'model_state_dict': model.state_dict(),\n",
        "    #         'optimizer_state_dict': optimizer.state_dict()}, \n",
        "\t  #   output_dir + str(epoch))\n",
        "  \n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
        "\n",
        "    losses = torch.cat(losses)\n",
        "    losses = losses[: len(eval_dataset)]\n",
        "\n",
        "    # perplexity metric used for mask language model training\n",
        "    try:\n",
        "        perplexity = math.exp(torch.mean(losses))\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
        "\n",
        "    # Save model\n",
        "    accelerator.wait_for_everyone()\n",
        "    torch.save({'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': perplexity}, \n",
        "\t    output_dir + str(epoch))\n",
        "\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "    # if val_loss < perplexity:\n",
        "    #   model.save_pretrained(output_dir)\n",
        "    #   tokenizer.save_pretrained(output_dir+ \"tokenizer.json\")\n",
        "    #   print(\"done training\")\n",
        "    #   break\n",
        "    val_loss = perplexity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b132ff9688f34d1c9aa42c7bc44d6ea4",
            "4551a530fe88452481a16c3e452c13f5",
            "b0079765d26f4e2c8f395308d180df0c",
            "39430bfdd688405ab88760258ef170f1",
            "76a99c286588471b84643a5a4e98361d",
            "b142aa906ab54187b7eaa0959da0f0da",
            "d85130dbaf85446899402f6b069d4886",
            "ab91a713275045dc9b0f9729d5bde313",
            "af137512a4d44c7a958b85364f1aaa9d",
            "f5347dae6bea4798aca391ff0777f306",
            "ed1419332507467eb4ff994ab35b90fe"
          ]
        },
        "id": "gQjkBbUf0ZT6",
        "outputId": "43680a90-da42-405f-ac2f-f727db26e3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/15430 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b132ff9688f34d1c9aa42c7bc44d6ea4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 0: Perplexity: 78.94696685527958\n",
            ">>> Epoch 1: Perplexity: 69.26831026735034\n",
            ">>> Epoch 2: Perplexity: 68.82121601422732\n",
            ">>> Epoch 3: Perplexity: 72.56297623521296\n",
            ">>> Epoch 4: Perplexity: 82.03366932844374\n",
            ">>> Epoch 5: Perplexity: 90.25296099530715\n",
            ">>> Epoch 6: Perplexity: 106.56402674146872\n",
            ">>> Epoch 7: Perplexity: 123.73160668308012\n",
            ">>> Epoch 8: Perplexity: 139.3609749092947\n",
            ">>> Epoch 9: Perplexity: 156.26365335801756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-7-1ee05687b3bc>\", line 50, in <module>\n",
            "    torch.save({'epoch': epoch,\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 422, in save\n",
            "    with _open_zipfile_writer(f) as opened_zipfile:\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 309, in _open_zipfile_writer\n",
            "    return container(name_or_buffer)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 287, in __init__\n",
            "    super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))\n",
            "RuntimeError: Parent directory gpt2_casual does not exist.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'RuntimeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 738, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 722, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 380, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save_pretrained(\"gpt2_casual\")\n",
        "tokenizer.save_pretrained(output_dir+ \"tokenizer.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "3L2szLbFsgCT",
        "outputId": "07200329-12f3-419e-a192-df6e32a52e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f0808e7ab1b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model.save_pretrained(\"gpt2_casual\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\"tokenizer.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output_dir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p3V61qSiIuYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"gpt2_mlm_model\")\n",
        "tokenizer.save_pretrained(\"gpt2_mlm_model/tokenizer.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XwSrokDL0lk",
        "outputId": "7a960206-7f7f-47b4-d138-1525849a5719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt2_mlm_model/tokenizer.json/tokenizer_config.json',\n",
              " 'gpt2_mlm_model/tokenizer.json/special_tokens_map.json',\n",
              " 'gpt2_mlm_model/tokenizer.json/vocab.json',\n",
              " 'gpt2_mlm_model/tokenizer.json/merges.txt',\n",
              " 'gpt2_mlm_model/tokenizer.json/added_tokens.json',\n",
              " 'gpt2_mlm_model/tokenizer.json/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, GPT2DoubleHeadsModel\n",
        "\n",
        "model = GPT2DoubleHeadsModel.from_pretrained('gpt2_mlm_average')\n",
        "model = model.to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2_mlm_average')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLazBBVGJJEo",
        "outputId": "98a7a191-b23b-4d9e-f742-c80d6d9a489b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "num_added_tokens = tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})\n",
        "# Update the model embeddings with the new vocabulary size\n",
        "# print(\"tokenizer length\", len(tokenizer))\n",
        "embedding_layer = model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "num_choices_arr = [2 , 5, 10, 25]\n",
        "\n",
        "for num_choices in num_choices_arr:\n",
        "\n",
        "  with open(f\"quiz_questions_{num_choices}.txt\") as f:\n",
        "    quiz_questions = json.load(f)\n",
        "  print(\"NUM_CHOICES:\", num_choices)\n",
        "\n",
        " \n",
        "\n",
        "  questions = quiz_questions\n",
        "  # answers = [0 for i in range(0, questions)]\n",
        "\n",
        "  counts = [0.0 for i in range(0, num_choices)]\n",
        "  total = 0.0\n",
        "  # print(\"question\", len(questions))\n",
        "  model.eval()\n",
        "  with torch.no_grad():        \n",
        "    for i, choices in enumerate(questions):\n",
        "        #print(choices)\n",
        "        # print(i)\n",
        "        if i % 1000 == 0 and i != 0:\n",
        "          print(f\"On iteration {i}, accuracy is {counts[0]/total} equal would be {1/num_choices} count is {total}\", [i/total for i in counts])\n",
        "        encoded_choices = [tokenizer.encode(s) for s in choices]\n",
        "        cls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]\n",
        "        #check if some choices are longer than the others\n",
        "        max_len = max([len(tokens) for tokens in encoded_choices])\n",
        "        check = True\n",
        "        for tokens in encoded_choices:\n",
        "            if len(tokens) < max_len:\n",
        "                check = False\n",
        "                break\n",
        "        if check:\n",
        "            input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\n",
        "            if len(input_ids) >= 1024:\n",
        "              continue\n",
        "            mc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1\n",
        "            input_ids = input_ids.to(device)\n",
        "            mc_token_ids = mc_token_ids.to(device)\n",
        "            outputs = model(input_ids, mc_token_ids=mc_token_ids)\n",
        "            lm_logits = outputs.logits\n",
        "            mc_logits = outputs.mc_logits\n",
        "            counts[mc_logits.argmax().item()] += 1\n",
        "            total += 1\n",
        "        torch.cuda.empty_cache() \n",
        "  print(f\"FINAL for num_choices {num_choices}: accuracy is {counts[0]/total} equal would be {1/num_choices} count is {total}\", [i/total for i in counts])\n",
        "\n",
        "  # print(counts)\n",
        "  # print(total)"
      ],
      "metadata": {
        "id": "C_8_HqChkiDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e42f42f-91d9-4651-aa82-ab1264eca282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NUM_CHOICES: 2\n",
            "On iteration 1000, accuracy is 0.47647647647647645 equal would be 0.5 count is 999.0 [0.47647647647647645, 0.5235235235235235]\n",
            "On iteration 2000, accuracy is 0.47183098591549294 equal would be 0.5 count is 1988.0 [0.47183098591549294, 0.528169014084507]\n",
            "On iteration 3000, accuracy is 0.4837412001340932 equal would be 0.5 count is 2983.0 [0.4837412001340932, 0.5162587998659068]\n",
            "On iteration 4000, accuracy is 0.4847951746670018 equal would be 0.5 count is 3979.0 [0.4847951746670018, 0.5152048253329983]\n",
            "On iteration 5000, accuracy is 0.4892505525416918 equal would be 0.5 count is 4977.0 [0.4892505525416918, 0.5107494474583082]\n",
            "On iteration 6000, accuracy is 0.4884538152610442 equal would be 0.5 count is 5976.0 [0.4884538152610442, 0.5115461847389559]\n",
            "On iteration 7000, accuracy is 0.490752688172043 equal would be 0.5 count is 6975.0 [0.490752688172043, 0.509247311827957]\n",
            "On iteration 8000, accuracy is 0.4890227073140133 equal would be 0.5 count is 7971.0 [0.4890227073140133, 0.5109772926859867]\n",
            "On iteration 9000, accuracy is 0.49041034790365745 equal would be 0.5 count is 8968.0 [0.49041034790365745, 0.5095896520963425]\n",
            "On iteration 10000, accuracy is 0.4902118261218753 equal would be 0.5 count is 9961.0 [0.4902118261218753, 0.5097881738781247]\n",
            "On iteration 11000, accuracy is 0.4892743039707896 equal would be 0.5 count is 10955.0 [0.4892743039707896, 0.5107256960292104]\n",
            "On iteration 12000, accuracy is 0.49000083675006273 equal would be 0.5 count is 11951.0 [0.49000083675006273, 0.5099991632499372]\n",
            "FINAL for num_choices 2: accuracy is 0.4913247265849848 equal would be 0.5 count is 12161.0 [0.4913247265849848, 0.5086752734150152]\n",
            "NUM_CHOICES: 5\n",
            "On iteration 1000, accuracy is 0.17753259779338015 equal would be 0.2 count is 997.0 [0.17753259779338015, 0.18655967903711135, 0.18756268806419257, 0.2246740220661986, 0.22367101303911735]\n",
            "On iteration 2000, accuracy is 0.17641129032258066 equal would be 0.2 count is 1984.0 [0.17641129032258066, 0.2001008064516129, 0.2091733870967742, 0.2086693548387097, 0.2056451612903226]\n",
            "On iteration 3000, accuracy is 0.19106783075889858 equal would be 0.2 count is 2978.0 [0.19106783075889858, 0.2028206850235057, 0.20349227669576897, 0.20315648085963733, 0.1994627266621894]\n",
            "On iteration 4000, accuracy is 0.18916876574307304 equal would be 0.2 count is 3970.0 [0.18916876574307304, 0.20503778337531486, 0.20327455919395465, 0.20377833753148614, 0.19874055415617128]\n",
            "On iteration 5000, accuracy is 0.19528415961305925 equal would be 0.2 count is 4962.0 [0.19528415961305925, 0.1977025392986699, 0.20495767835550183, 0.20193470374848851, 0.20012091898428053]\n",
            "On iteration 6000, accuracy is 0.19848993288590605 equal would be 0.2 count is 5960.0 [0.19848993288590605, 0.19916107382550335, 0.20385906040268456, 0.19916107382550335, 0.19932885906040268]\n",
            "On iteration 7000, accuracy is 0.19867740080506038 equal would be 0.2 count is 6956.0 [0.19867740080506038, 0.19824611845888443, 0.199252443933295, 0.20097757331799884, 0.20284646348476135]\n",
            "On iteration 8000, accuracy is 0.1969277260135986 equal would be 0.2 count is 7942.0 [0.1969277260135986, 0.19780911609166457, 0.20183832787710904, 0.2010828506673382, 0.2023419793502896]\n",
            "On iteration 9000, accuracy is 0.19856727109917172 equal would be 0.2 count is 8934.0 [0.19856727109917172, 0.19834340720841728, 0.2005820461159615, 0.20136556973360198, 0.20114170584284755]\n",
            "On iteration 10000, accuracy is 0.19832711881487453 equal would be 0.2 count is 9923.0 [0.19832711881487453, 0.19832711881487453, 0.20255970976519197, 0.20074574221505592, 0.20004031039000303]\n",
            "On iteration 11000, accuracy is 0.19807604214383875 equal would be 0.2 count is 10915.0 [0.19807604214383875, 0.19798442510306918, 0.20018323408153918, 0.20192395785616124, 0.20183234081539167]\n",
            "On iteration 12000, accuracy is 0.19847135897866622 equal would be 0.2 count is 11906.0 [0.19847135897866622, 0.19754745506467328, 0.20040315807156056, 0.2013270619855535, 0.20225096589954644]\n",
            "FINAL for num_choices 5: accuracy is 0.19859678085018573 equal would be 0.2 count is 12115.0 [0.19859678085018573, 0.19702847709451093, 0.2008254230293025, 0.20148576145274452, 0.20206355757325628]\n",
            "NUM_CHOICES: 10\n",
            "On iteration 1000, accuracy is 0.08274470232088799 equal would be 0.1 count is 991.0 [0.08274470232088799, 0.09989909182643794, 0.09889001009081735, 0.1170534813319879, 0.08980827447023208, 0.10191725529767912, 0.09889001009081735, 0.10797174571140263, 0.11301715438950555, 0.08980827447023208]\n",
            "On iteration 2000, accuracy is 0.08581524482584553 equal would be 0.1 count is 1981.0 [0.08581524482584553, 0.09843513377082282, 0.09641595153962645, 0.10853104492680464, 0.09389197375063099, 0.09742554265522463, 0.10398788490661282, 0.10752145381120647, 0.11357900050479555, 0.09439676930843009]\n",
            "On iteration 3000, accuracy is 0.09152086137281291 equal would be 0.1 count is 2972.0 [0.09152086137281291, 0.09522207267833109, 0.09724091520861373, 0.10800807537012114, 0.09656796769851951, 0.10397039030955585, 0.09757738896366083, 0.10296096904441454, 0.11372812920592194, 0.09320323014804845]\n",
            "On iteration 4000, accuracy is 0.08856926570779712 equal would be 0.1 count is 3963.0 [0.08856926570779712, 0.09613928841786526, 0.0986626293212213, 0.10724198839263184, 0.09664395659853646, 0.10244764067625536, 0.09563462023719405, 0.10774665657330305, 0.1102699974766591, 0.09664395659853646]\n",
            "On iteration 5000, accuracy is 0.08923884514435695 equal would be 0.1 count is 4953.0 [0.08923884514435695, 0.09307490409852615, 0.10175651120533011, 0.10599636583888553, 0.09953563496870584, 0.10155461336563698, 0.10155461336563698, 0.10478497880072683, 0.10397738744195437, 0.09852614577024026]\n",
            "On iteration 6000, accuracy is 0.09136799596163554 equal would be 0.1 count is 5943.0 [0.09136799596163554, 0.09675248191149251, 0.1033148241628807, 0.10482921083627797, 0.09927645970048797, 0.10297829379101464, 0.09961299007235402, 0.10398788490661282, 0.10146390711761737, 0.09641595153962645]\n",
            "On iteration 7000, accuracy is 0.0929662727010666 equal would be 0.1 count is 6938.0 [0.0929662727010666, 0.09599308157970597, 0.10478524070337274, 0.10247910060536178, 0.09786682040933986, 0.09829922167771692, 0.1004612280196022, 0.10608244450850389, 0.10377630441049293, 0.09729028538483712]\n",
            "On iteration 8000, accuracy is 0.09255050505050505 equal would be 0.1 count is 7920.0 [0.09255050505050505, 0.09747474747474748, 0.1053030303030303, 0.10303030303030303, 0.09785353535353536, 0.09772727272727273, 0.09962121212121212, 0.10643939393939394, 0.10239898989898989, 0.0976010101010101]\n",
            "On iteration 9000, accuracy is 0.0921023109715055 equal would be 0.1 count is 8914.0 [0.0921023109715055, 0.0995063944357191, 0.106237379403186, 0.10107695759479471, 0.09916984518734574, 0.09894547902176352, 0.09872111285618129, 0.10444245007852816, 0.1025353376710792, 0.09726273277989679]\n",
            "On iteration 10000, accuracy is 0.09223153853924639 equal would be 0.1 count is 9899.0 [0.09223153853924639, 0.09980806142034548, 0.10465703606424892, 0.10122234569148399, 0.09869683806445095, 0.09950500050510153, 0.09879785836953228, 0.10445499545408628, 0.1027376502677038, 0.09788867562380038]\n",
            "On iteration 11000, accuracy is 0.09150206706476803 equal would be 0.1 count is 10885.0 [0.09150206706476803, 0.10142397795130914, 0.10372071658245292, 0.10142397795130914, 0.09903536977491961, 0.10096463022508038, 0.09866789159393662, 0.10390445567294442, 0.10252641249425816, 0.09683050068902159]\n",
            "On iteration 12000, accuracy is 0.09181266846361186 equal would be 0.1 count is 11872.0 [0.09181266846361186, 0.10183625336927224, 0.1042789757412399, 0.09998315363881402, 0.09880390835579515, 0.10116239892183289, 0.09998315363881402, 0.10394204851752022, 0.10133086253369272, 0.09686657681940701]\n",
            "FINAL for num_choices 10: accuracy is 0.09147350993377483 equal would be 0.1 count is 12080.0 [0.09147350993377483, 0.1015728476821192, 0.10422185430463576, 0.09966887417218544, 0.09884105960264901, 0.1017384105960265, 0.09966887417218544, 0.1037251655629139, 0.10198675496688742, 0.09710264900662252]\n",
            "NUM_CHOICES: 25\n",
            "On iteration 1000, accuracy is 0.03340080971659919 equal would be 0.04 count is 988.0 [0.03340080971659919, 0.03744939271255061, 0.039473684210526314, 0.04959514170040486, 0.039473684210526314, 0.044534412955465584, 0.05465587044534413, 0.0465587044534413, 0.03137651821862348, 0.03137651821862348, 0.039473684210526314, 0.043522267206477734, 0.030364372469635626, 0.03643724696356275, 0.043522267206477734, 0.038461538461538464, 0.04251012145748988, 0.03340080971659919, 0.04048582995951417, 0.04048582995951417, 0.04959514170040486, 0.04048582995951417, 0.039473684210526314, 0.032388663967611336, 0.04149797570850203]\n",
            "On iteration 2000, accuracy is 0.034552845528455285 equal would be 0.04 count is 1968.0 [0.034552845528455285, 0.039634146341463415, 0.03709349593495935, 0.043191056910569105, 0.036077235772357726, 0.04217479674796748, 0.05030487804878049, 0.0508130081300813, 0.04065040650406504, 0.03201219512195122, 0.038109756097560975, 0.04522357723577236, 0.038109756097560975, 0.036585365853658534, 0.04014227642276423, 0.041666666666666664, 0.041666666666666664, 0.036077235772357726, 0.038109756097560975, 0.04065040650406504, 0.04369918699186992, 0.032520325203252036, 0.042682926829268296, 0.036585365853658534, 0.041666666666666664]\n",
            "On iteration 3000, accuracy is 0.035545023696682464 equal would be 0.04 count is 2954.0 [0.035545023696682464, 0.04129993229519296, 0.037914691943127965, 0.04536222071767095, 0.037914691943127965, 0.042315504400812456, 0.04908598510494245, 0.04739336492890995, 0.03960731211916046, 0.02945159106296547, 0.03656059580230196, 0.04468517264725796, 0.037914691943127965, 0.037914691943127965, 0.037914691943127965, 0.04265402843601896, 0.03994583615436696, 0.03351387948544347, 0.03994583615436696, 0.045700744752877456, 0.04468517264725796, 0.03419092755585647, 0.041638456330399455, 0.03656059580230196, 0.04028436018957346]\n",
            "On iteration 4000, accuracy is 0.0340620233858668 equal would be 0.04 count is 3934.0 [0.0340620233858668, 0.04041687849517031, 0.04092526690391459, 0.044229791560752414, 0.03838332486019319, 0.04194204372140315, 0.04753431621759024, 0.04499237417386884, 0.03889171326893747, 0.0340620233858668, 0.03812913065582105, 0.041687849517031014, 0.03711235383833249, 0.04067107269954245, 0.03812913065582105, 0.04041687849517031, 0.03838332486019319, 0.035332994407727504, 0.041687849517031014, 0.04499237417386884, 0.04245043213014743, 0.036858159633960344, 0.042704626334519574, 0.03660396542958821, 0.03940010167768175]\n",
            "On iteration 5000, accuracy is 0.035401831129196336 equal would be 0.04 count is 4915.0 [0.035401831129196336, 0.04231943031536114, 0.04231943031536114, 0.04374364191251272, 0.03845371312309257, 0.04191251271617497, 0.044760935910478125, 0.044150559511698884, 0.037029501525941, 0.03438453713123093, 0.03763987792472025, 0.04150559511698881, 0.038657171922685654, 0.04191251271617497, 0.03825025432349949, 0.039471007121057985, 0.037436419125127164, 0.03723296032553408, 0.040895218718209565, 0.042115971515768055, 0.04170905391658189, 0.03784333672431333, 0.04150559511698881, 0.038657171922685654, 0.04069175991861648]\n",
            "On iteration 6000, accuracy is 0.03590177815410669 equal would be 0.04 count is 5905.0 [0.03590177815410669, 0.04335309060118544, 0.04132091447925487, 0.04419983065198984, 0.03962743437764606, 0.041151566469093986, 0.04521591871295512, 0.04521591871295512, 0.03590177815410669, 0.03556308213378493, 0.037933954276037254, 0.04098221845893311, 0.03810330228619814, 0.041998306519898394, 0.0384419983065199, 0.03810330228619814, 0.037087214225232853, 0.0392887383573243, 0.039458086367485184, 0.041998306519898394, 0.040304826418289585, 0.03640982218458933, 0.04132091447925487, 0.04013547840812871, 0.04098221845893311]\n",
            "On iteration 7000, accuracy is 0.038020606588303585 equal would be 0.04 count is 6891.0 [0.038020606588303585, 0.04382527934987665, 0.04077782615005079, 0.04208387752140473, 0.039616891597736174, 0.0413582934262081, 0.043535045711798, 0.04542156435930925, 0.03671455521694964, 0.03584385430271368, 0.038891307502539545, 0.04092294296909012, 0.0375852561311856, 0.04193876070236541, 0.0391815411406182, 0.03845595704542156, 0.037149905674067625, 0.038746190683500215, 0.039762008416775504, 0.041648527064286754, 0.039616891597736174, 0.037875489769264255, 0.0413582934262081, 0.0391815411406182, 0.040487592511972134]\n",
            "On iteration 8000, accuracy is 0.036354391763060885 equal would be 0.04 count is 7867.0 [0.036354391763060885, 0.04436252701156731, 0.0408033557900089, 0.0406762425320961, 0.039659336468793695, 0.04385407397991611, 0.042710054658700904, 0.04525231981695691, 0.036100165247235284, 0.03698995805262489, 0.03915088343714249, 0.0405491292741833, 0.03851531714757849, 0.0415660353374857, 0.039532223210880894, 0.03813397737384009, 0.03826109063175289, 0.039532223210880894, 0.03787975085801449, 0.0405491292741833, 0.039405109952968094, 0.03787975085801449, 0.042710054658700904, 0.03889665692131689, 0.0406762425320961]\n",
            "On iteration 9000, accuracy is 0.035952515545505936 equal would be 0.04 count is 8845.0 [0.035952515545505936, 0.04465799886941775, 0.040474844544940644, 0.041266252119841716, 0.040814019219898245, 0.042509892594686266, 0.04228377614471453, 0.04420576596947428, 0.03708309779536461, 0.03651780667043528, 0.03945732052006783, 0.040927077444884115, 0.03855285472018089, 0.0423968343697004, 0.03934426229508197, 0.03753533069530808, 0.03889202939513849, 0.03889202939513849, 0.03753533069530808, 0.040474844544940644, 0.040474844544940644, 0.03821368004522329, 0.041944601469756926, 0.04013566986998304, 0.03945732052006783]\n",
            "On iteration 10000, accuracy is 0.035212700997353956 equal would be 0.04 count is 9826.0 [0.035212700997353956, 0.044779157337675554, 0.03948707510685935, 0.040606554040301245, 0.041013637288825566, 0.042540199470791774, 0.04325259515570934, 0.04284551190718502, 0.03846936698554854, 0.03684103399145125, 0.03999592916751476, 0.04172603297374313, 0.03806228373702422, 0.04274374109505394, 0.03857113779767962, 0.03785874211276206, 0.03918176267046611, 0.03907999185833503, 0.03796051292489314, 0.04009769997964584, 0.0408100956645634, 0.037553429676368814, 0.041115408100956645, 0.04009769997964584, 0.04009769997964584]\n",
            "On iteration 11000, accuracy is 0.03424023690542291 equal would be 0.04 count is 10806.0 [0.03424023690542291, 0.045715343327780864, 0.03960762539330002, 0.04044049602072922, 0.04016287247825282, 0.04081066074403109, 0.04441976679622432, 0.042106237275587634, 0.03858967240421988, 0.03720155469183787, 0.040347954839903756, 0.04090320192485656, 0.03812696650009254, 0.04201369609476217, 0.03858967240421988, 0.038404590042568944, 0.039700166574125485, 0.04016287247825282, 0.037386637053488804, 0.040070331297427354, 0.04145844900980936, 0.03729409587266334, 0.041365907828983896, 0.04044049602072922, 0.04044049602072922]\n",
            "On iteration 12000, accuracy is 0.03529311953847459 equal would be 0.04 count is 11787.0 [0.03529311953847459, 0.045728344786629335, 0.03928056333248494, 0.04046831254772207, 0.03970475948078391, 0.041147026385000424, 0.04326800712649529, 0.04326800712649529, 0.03868668872486638, 0.03724442182064987, 0.040213794858742685, 0.04012895562908289, 0.03792313565792823, 0.04191057945193857, 0.03945024179180453, 0.038432171035886996, 0.03961992025112412, 0.040044116399423096, 0.037414100279969456, 0.040977347925680835, 0.04089250869602104, 0.03724442182064987, 0.04131670484432001, 0.04029863408840248, 0.040044116399423096]\n",
            "FINAL for num_choices 25: accuracy is 0.035345115038346114 equal would be 0.04 count is 11996.0 [0.035345115038346114, 0.04543181060353451, 0.03926308769589863, 0.0405135045015005, 0.04009669889963321, 0.04093031010336779, 0.04334778259419807, 0.043431143714571525, 0.039096365455151716, 0.03701233744581527, 0.04043014338112704, 0.0399299766588863, 0.037595865288429474, 0.04193064354784928, 0.03926308769589863, 0.0383461153717906, 0.039596532177392466, 0.04001333777925975, 0.037595865288429474, 0.04101367122374125, 0.04101367122374125, 0.037595865288429474, 0.04151383794598199, 0.03984661553851284, 0.03984661553851284]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "num_added_tokens = tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})\n",
        "# Update the model embeddings with the new vocabulary size\n",
        "# print(\"tokenizer length\", len(tokenizer))\n",
        "embedding_layer = model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "num_choices_arr = [5]\n",
        "model_ans = []\n",
        "for num_choices in num_choices_arr:\n",
        "\n",
        "  with open(f\"quiz_questions_{num_choices}.txt\") as f:\n",
        "    quiz_questions = json.load(f)\n",
        "  print(\"NUM_CHOICES:\", num_choices)\n",
        "\n",
        " \n",
        "\n",
        "  questions = quiz_questions\n",
        "  # answers = [0 for i in range(0, questions)]\n",
        "\n",
        "  counts = [0.0 for i in range(0, num_choices)]\n",
        "  total = 0.0\n",
        "  # print(\"question\", len(questions))\n",
        "  model.eval()\n",
        "  with torch.no_grad():        \n",
        "    for i, choices in enumerate(questions):\n",
        "        #print(choices)\n",
        "        # print(i)\n",
        "        if i % 1000 == 0 and i != 0:\n",
        "          print(f\"On iteration {i}, model_ans length is {len(model_ans)}, accuracy is {counts[0]/total} equal would be {1/num_choices} count is {total}\", [i/total for i in counts])\n",
        "        encoded_choices = [tokenizer.encode(s) for s in choices]\n",
        "        cls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]\n",
        "        #check if some choices are longer than the others\n",
        "        max_len = max([len(tokens) for tokens in encoded_choices])\n",
        "        check = True\n",
        "        for tokens in encoded_choices:\n",
        "            if len(tokens) < max_len:\n",
        "                check = False\n",
        "                break\n",
        "        if check:\n",
        "            input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\n",
        "            if len(input_ids) >= 1024:\n",
        "              continue\n",
        "            mc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1\n",
        "            input_ids = input_ids.to(device)\n",
        "            mc_token_ids = mc_token_ids.to(device)\n",
        "            outputs = model(input_ids, mc_token_ids=mc_token_ids)\n",
        "            lm_logits = outputs.logits\n",
        "            mc_logits = outputs.mc_logits\n",
        "            counts[mc_logits.argmax().item()] += 1\n",
        "            total += 1\n",
        "            model_ans.append(mc_logits.argmax().item())\n",
        "        else:\n",
        "          model_ans.append(-1)\n",
        "        torch.cuda.empty_cache() \n",
        "  print(f\"FINAL for num_choices {num_choices}: accuracy is {counts[0]/total} equal would be {1/num_choices} count is {total}\", [i/total for i in counts])\n",
        "\n",
        "  # print(counts)\n",
        "  # print(total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufQtF3Jvip21",
        "outputId": "18d271b1-4ec9-4751-e968-7535e045e3f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NUM_CHOICES: 5\n",
            "On iteration 1000, model_ans length is 1000, accuracy is 0.1995987963891675 equal would be 0.2 count is 997.0 [0.1995987963891675, 0.20661985957873621, 0.20361083249749248, 0.20260782347041123, 0.18756268806419257]\n",
            "On iteration 2000, model_ans length is 2000, accuracy is 0.20211693548387097 equal would be 0.2 count is 1984.0 [0.20211693548387097, 0.20967741935483872, 0.19606854838709678, 0.20715725806451613, 0.1849798387096774]\n",
            "On iteration 3000, model_ans length is 3000, accuracy is 0.19879113498992612 equal would be 0.2 count is 2978.0 [0.19879113498992612, 0.20953660174613833, 0.19677635997313633, 0.20852921423774345, 0.18636668905305573]\n",
            "On iteration 4000, model_ans length is 4000, accuracy is 0.2035264483627204 equal would be 0.2 count is 3970.0 [0.2035264483627204, 0.20554156171284635, 0.19748110831234256, 0.2047858942065491, 0.18866498740554155]\n",
            "On iteration 5000, model_ans length is 5000, accuracy is 0.20395002015316405 equal would be 0.2 count is 4962.0 [0.20395002015316405, 0.20435308343409916, 0.198105602579605, 0.2015316404675534, 0.1920596533655784]\n",
            "On iteration 6000, model_ans length is 6000, accuracy is 0.2011744966442953 equal would be 0.2 count is 5960.0 [0.2011744966442953, 0.20738255033557046, 0.20033557046979866, 0.20033557046979866, 0.19077181208053692]\n",
            "On iteration 7000, model_ans length is 7000, accuracy is 0.20025876940770557 equal would be 0.2 count is 6956.0 [0.20025876940770557, 0.20629672225416906, 0.20040253018976423, 0.20126509488211616, 0.19177688326624498]\n",
            "On iteration 8000, model_ans length is 8000, accuracy is 0.1988164190380257 equal would be 0.2 count is 7942.0 [0.1988164190380257, 0.2058675396625535, 0.19818685469654998, 0.20372702090153613, 0.19340216570133467]\n",
            "On iteration 9000, model_ans length is 9000, accuracy is 0.1975598835907768 equal would be 0.2 count is 8934.0 [0.1975598835907768, 0.2038280725319006, 0.20114170584284755, 0.20349227669576897, 0.19397806133870607]\n",
            "On iteration 10000, model_ans length is 10000, accuracy is 0.19762168698982163 equal would be 0.2 count is 9923.0 [0.19762168698982163, 0.20235815781517685, 0.19943565453995768, 0.20477678121535825, 0.19580771943968558]\n",
            "On iteration 11000, model_ans length is 11000, accuracy is 0.19596885020613833 equal would be 0.2 count is 10915.0 [0.19596885020613833, 0.20503893724232708, 0.20009161704076958, 0.20302336234539625, 0.19587723316536876]\n",
            "On iteration 12000, model_ans length is 12000, accuracy is 0.19838736771375776 equal would be 0.2 count is 11906.0 [0.19838736771375776, 0.20225096589954644, 0.20073912313119435, 0.2035108348731732, 0.19511170838232825]\n",
            "FINAL for num_choices 5: accuracy is 0.19834915394139496 equal would be 0.2 count is 12115.0 [0.19834915394139496, 0.20132067684688404, 0.20206355757325628, 0.20346677672307056, 0.19479983491539413]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"compare_5/pretrain_average.txt\", \"w+\")\n",
        "for i in model_ans:\n",
        "  f.write(str(i)+\"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "86PctXvXi3AR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}