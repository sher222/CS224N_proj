{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "64f2ac1621074091819cb218a265da55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20dfc35f51d94a898baa092e7b3130d1",
              "IPY_MODEL_56d10632c85a4531851461e42016f613",
              "IPY_MODEL_4671a8e7b46142aa9bd50f01138ce8bf"
            ],
            "layout": "IPY_MODEL_6558c62368194cd894d5e83f3f77068b"
          }
        },
        "20dfc35f51d94a898baa092e7b3130d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00116a13f7e449ae8055df7fe9c1fbf4",
            "placeholder": "​",
            "style": "IPY_MODEL_5383013fbc51409cb3fe4cc391123751",
            "value": " 70%"
          }
        },
        "56d10632c85a4531851461e42016f613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f876109f67724f17b0142d47d51e87c1",
            "max": 15430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4419d8dc1cff4819a768bd5b4aef43ea",
            "value": 10802
          }
        },
        "4671a8e7b46142aa9bd50f01138ce8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91688c70035c40aa99eb8ec64940171a",
            "placeholder": "​",
            "style": "IPY_MODEL_3c1533c67bed41b6bc4b96b474abb019",
            "value": " 10801/15430 [1:45:15&lt;38:39,  2.00it/s]"
          }
        },
        "6558c62368194cd894d5e83f3f77068b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00116a13f7e449ae8055df7fe9c1fbf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5383013fbc51409cb3fe4cc391123751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f876109f67724f17b0142d47d51e87c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4419d8dc1cff4819a768bd5b4aef43ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91688c70035c40aa99eb8ec64940171a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c1533c67bed41b6bc4b96b474abb019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58145f8660f146b892bc0933fadba2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3789c3726c3b49b9816bd77c6acb5baf",
              "IPY_MODEL_cc7b0dcfe10e4852b7b7f986edd76e9b",
              "IPY_MODEL_a51f9e11c1054d84a38cbe084fa4d135"
            ],
            "layout": "IPY_MODEL_410080a5ff1f43bdbe5536502f1d63ee"
          }
        },
        "3789c3726c3b49b9816bd77c6acb5baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a46fb23b6421424a8644b2acf8ef3138",
            "placeholder": "​",
            "style": "IPY_MODEL_45fadf14f33d4c4db81cc33afa68ccb1",
            "value": "100%"
          }
        },
        "cc7b0dcfe10e4852b7b7f986edd76e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4cbcf8f639743448440c43e0c5be936",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc3a8085430246ebb713b82393caba48",
            "value": 2
          }
        },
        "a51f9e11c1054d84a38cbe084fa4d135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae8cb302a6c443d0aaa2bc914956f77b",
            "placeholder": "​",
            "style": "IPY_MODEL_a284a81612044fa5b214c97bece3d2b6",
            "value": " 2/2 [00:00&lt;00:00, 71.43it/s]"
          }
        },
        "410080a5ff1f43bdbe5536502f1d63ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a46fb23b6421424a8644b2acf8ef3138": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45fadf14f33d4c4db81cc33afa68ccb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4cbcf8f639743448440c43e0c5be936": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc3a8085430246ebb713b82393caba48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae8cb302a6c443d0aaa2bc914956f77b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a284a81612044fa5b214c97bece3d2b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDEPXsyw07zr",
        "outputId": "7faf96ab-bd5c-4805-b758-a5a296d62876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: adapter-transformers in /usr/local/lib/python3.9/dist-packages (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from adapter-transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from adapter-transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from adapter-transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from adapter-transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from adapter-transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from adapter-transformers) (3.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from adapter-transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from adapter-transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from adapter-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->adapter-transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->adapter-transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->adapter-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->adapter-transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->adapter-transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (0.17.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.13.1+cu116)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install adapter-transformers\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vio6AhLq3YAX",
        "outputId": "a9cc8ae2-45db-40fc-89e7-b90c0c2d8515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0FNupPjFJHA",
        "outputId": "97e0e503-f1e0-458f-c119-5f4e05e55486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/data/\n",
        "# ! rm gpt2_adapter_casual\n",
        "# ! mkdir gpt2_adapter_casual\n",
        "! ls # verify that you are in the right directory"
      ],
      "metadata": {
        "id": "lVoH3D3aCpL3",
        "outputId": "34e8c452-13a1-49f3-f40f-08a826031e79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data\n",
            "finalWords.json\t\t  gpt2_predict_model\n",
            "finetune_eval.txt\t  gpt2_predict_model_30\n",
            "finetune_train.txt\t  gpt2_predict_model_6\n",
            "finetune_val.txt\t  predict_embeddings_model_weights\n",
            "gpt2_adapter_casual\t  predict_embeddings_model_weights_30\n",
            "gpt2_adapter_mlm_model\t  predict_embeddings_model_weights_6\n",
            "gpt2_adapter_mlm_model_3  quiz_questions_10.txt\n",
            "gpt2_casual\t\t  quiz_questions_25.txt\n",
            "gpt2_casual_2\t\t  quiz_questions_2.txt\n",
            "gpt2_finetune_random\t  quiz_questions_50.txt\n",
            "gpt2_mlm_model\t\t  quiz_questions_5.txt\n",
            "gpt2_mlm_model_2\t  quiz_questions.txt\n",
            "gpt2_mlm_model_3\t  words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, GPT2DoubleHeadsModel\n",
        "import torch\n",
        "import json\n",
        "\n",
        "\n",
        "# Load the JSON file\n",
        "with open('finalWords.json') as f:\n",
        "    words = json.load(f)\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2DoubleHeadsModel.from_pretrained(\"gpt2\")\n",
        "model = model.to(device)\n",
        "\n",
        "original_vocab_size = len(tokenizer)\n",
        "new_word_list = [i.strip().lower() for i in open(\"words.txt\").readlines()]\n",
        "num_new_words = len(new_word_list)\n",
        "num_added = tokenizer.add_tokens(new_word_list)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "#Compute the distribution from which we’ll sample:\n",
        "params = model.state_dict()\n",
        "embeddings = params['transformer.wte.weight']\n",
        "pre_expansion_embeddings = embeddings[:-num_new_words,:]\n",
        "mu = torch.mean(pre_expansion_embeddings, dim=0)\n",
        "n = pre_expansion_embeddings.size()[0]\n",
        "sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n\n",
        "dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
        "        mu, covariance_matrix=1e-5*sigma)\n",
        "\n",
        "#load in our new embeddings into the model:\n",
        "new_embeddings = torch.stack(tuple((dist.sample() for _ in range(num_new_words))), dim=0)\n",
        "embeddings[-num_new_words:,:] = new_embeddings\n",
        "params['transformer.wte.weight'][-num_new_words:,:] = new_embeddings\n",
        "model.load_state_dict(params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvyBmJ5I1EUv",
        "outputId": "27040135-467f-4c96-cf93-18015dbd018e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing GPT2DoubleHeadsModel.\n",
            "\n",
            "Some weights of GPT2DoubleHeadsModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['multiple_choice_head.summary.weight', 'multiple_choice_head.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.add_adapter(\"adapter\", config=\"pfeiffer+inv\")\n",
        "model.train_adapter(\"adapter\")"
      ],
      "metadata": {
        "id": "sAtRDHWk_dl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aed4faa-f399-444e-d9bb-6bf2d8aee3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Adding adapter 'adapter'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "text = load_dataset(\"text\",data_files={\"train\": \"finetune_train.txt\", \"test\": \"finetune_val.txt\"})\n",
        "# print(text['train'])\n",
        "# text = text['train'].train_test_split(test_size=0.2)\n",
        "# print(text[\"train\"][0:10])\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer([x for x in examples[\"text\"]], truncation=True)\n",
        "tokenized_txt = text.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns=text[\"train\"].column_names\n",
        ")\n",
        "# print(tokenized_txt[\"train \"][0:100])\n",
        "block_size = 128\n",
        "\n",
        "# print(tokenizer(\"My wife had a PMS Strike last night. PMS strikes are ruining my marriage!\"))\n",
        "def group_texts(examples):\n",
        "    # print(examples)\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "lm_dataset = tokenized_txt.map(group_texts, batched=True, num_proc=4)\n"
      ],
      "metadata": {
        "id": "Ti5hbH3g1qKh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154,
          "referenced_widgets": [
            "58145f8660f146b892bc0933fadba2cd",
            "3789c3726c3b49b9816bd77c6acb5baf",
            "cc7b0dcfe10e4852b7b7f986edd76e9b",
            "a51f9e11c1054d84a38cbe084fa4d135",
            "410080a5ff1f43bdbe5536502f1d63ee",
            "a46fb23b6421424a8644b2acf8ef3138",
            "45fadf14f33d4c4db81cc33afa68ccb1",
            "f4cbcf8f639743448440c43e0c5be936",
            "dc3a8085430246ebb713b82393caba48",
            "ae8cb302a6c443d0aaa2bc914956f77b",
            "a284a81612044fa5b214c97bece3d2b6"
          ]
        },
        "outputId": "98452222-799b-4f46-a699-a3d3c2693f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-44d754cc7e7fe124/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58145f8660f146b892bc0933fadba2cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-44d754cc7e7fe124/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-add71312ab63cc0a_*_of_00004.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-44d754cc7e7fe124/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-089bf743f92183db_*_of_00004.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-44d754cc7e7fe124/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-32cb954df5e3beec_*_of_00004.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-44d754cc7e7fe124/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-4dee680c64e12078_*_of_00004.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.mask_token = u\"\\u2047\"\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "MNDS6Doz_C8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import TrainingArguments, AdapterTrainer\n",
        "\n",
        "# model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2_adapter_casual\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_steps = 100000000,\n",
        "    learning_rate=3e-4,\n",
        "    num_train_epochs=12,\n",
        "    weight_decay=0.1,\n",
        "    logging_steps=200\n",
        ")\n",
        "\n",
        "trainer = AdapterTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_dataset[\"train\"],\n",
        "    eval_dataset=lm_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"gpt2_adapter_casual\")"
      ],
      "metadata": {
        "id": "NF9vtpnt1vt8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d05a3cf9-4778-4cb5-d6e8-98eaf48198bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "***** Running training *****\n",
            "  Num examples = 24682\n",
            "  Num Epochs = 12\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 37032\n",
            "  Number of trainable parameters = 1191361\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37032' max='37032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37032/37032 2:56:43, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5.345900</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>5.256200</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>5.224900</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5.171800</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>5.164900</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>5.117100</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>5.113800</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>5.123100</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>5.106600</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.098200</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>5.071100</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>5.065800</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 2873\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2873\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2873\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2873\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2873\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2873\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2873\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2873\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2873\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2873\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2873\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2873\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to gpt2_adapter_casual\n",
            "Configuration saved in gpt2_adapter_casual/adapter/adapter_config.json\n",
            "Module weights saved in gpt2_adapter_casual/adapter/pytorch_adapter.bin\n",
            "Configuration saved in gpt2_adapter_casual/adapter/head_config.json\n",
            "Module weights saved in gpt2_adapter_casual/adapter/pytorch_model_head.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/ayoolaolafenwa/TrainNLP\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(lm_dataset[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator,)\n",
        "\n",
        "# load the test dataset for evaluation\n",
        "eval_dataloader = DataLoader(lm_dataset[\"test\"], batch_size=batch_size, collate_fn=data_collator)\n",
        "eval_dataset = lm_dataset[\"test\"]\n",
        "# initialize pretrained bert model\n",
        "# model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "# set the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "# initialize accelerator for training\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
        "\n",
        "# set the number of epochs which is set to 30\n",
        "num_train_epochs = 10\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "# define the learning rate scheduler for training\n",
        "lr_scheduler = get_scheduler(\"linear\",optimizer=optimizer,num_warmup_steps=0,num_training_steps=num_training_steps)\n"
      ],
      "metadata": {
        "id": "jEknXVu5-UwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model = GPT2DoubleHeadsModel.from_pretrained(\"gpt2\")\n",
        "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "checkpoint = torch.load(\"gpt2_adapter_casual/6\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']"
      ],
      "metadata": {
        "id": "G5XCSrw_NoCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "# directory to save the models\n",
        "output_dir = \"gpt2_adapter_casual/\"\n",
        "val_loss = 1000000\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
        "\n",
        "    losses = torch.cat(losses)\n",
        "    losses = losses[: len(eval_dataset)]\n",
        "\n",
        "    # perplexity metric used for mask language model training\n",
        "    try:\n",
        "        perplexity = math.exp(torch.mean(losses))\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
        "\n",
        "    # Save model\n",
        "    accelerator.wait_for_everyone()\n",
        "    torch.save({'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': perplexity}, \n",
        "  output_dir + str(epoch))\n",
        "    if val_loss < perplexity:\n",
        "      # model.save_pretrained(output_dir)\n",
        "      # tokenizer.save_pretrained(output_dir+ \"/tokenizer.json\")\n",
        "      print(\"done training\")\n",
        "      break\n",
        "    val_loss = perplexity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "64f2ac1621074091819cb218a265da55",
            "20dfc35f51d94a898baa092e7b3130d1",
            "56d10632c85a4531851461e42016f613",
            "4671a8e7b46142aa9bd50f01138ce8bf",
            "6558c62368194cd894d5e83f3f77068b",
            "00116a13f7e449ae8055df7fe9c1fbf4",
            "5383013fbc51409cb3fe4cc391123751",
            "f876109f67724f17b0142d47d51e87c1",
            "4419d8dc1cff4819a768bd5b4aef43ea",
            "91688c70035c40aa99eb8ec64940171a",
            "3c1533c67bed41b6bc4b96b474abb019"
          ]
        },
        "id": "Fw5vlVJz_N3R",
        "outputId": "97729a7a-6fb1-472a-fb58-034063b352f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/15430 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64f2ac1621074091819cb218a265da55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 0: Perplexity: 170.20959705920922\n",
            ">>> Epoch 1: Perplexity: 167.15081427469474\n",
            ">>> Epoch 2: Perplexity: 165.91600504022688\n",
            ">>> Epoch 3: Perplexity: 164.31035525451045\n",
            ">>> Epoch 4: Perplexity: 163.57466210492646\n",
            ">>> Epoch 5: Perplexity: 162.83636175701213\n",
            ">>> Epoch 6: Perplexity: 162.32106301065207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_adapter(\"gpt2_adapter_casual\", \"adapter\", with_head=True)\n",
        "tokenizer.save_pretrained(\"gpt2_adapter_casual/tokenizer.json\")"
      ],
      "metadata": {
        "id": "j1P1qmAmPFJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba79e057-462f-4a86-c359-1c71f1bf297e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in gpt2_adapter_casual/adapter_config.json\n",
            "Module weights saved in gpt2_adapter_casual/pytorch_adapter.bin\n",
            "Configuration saved in gpt2_adapter_casual/head_config.json\n",
            "Module weights saved in gpt2_adapter_casual/pytorch_model_head.bin\n",
            "tokenizer config file saved in gpt2_adapter_casual/tokenizer.json/tokenizer_config.json\n",
            "Special tokens file saved in gpt2_adapter_casual/tokenizer.json/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt2_adapter_casual/tokenizer.json/tokenizer_config.json',\n",
              " 'gpt2_adapter_casual/tokenizer.json/special_tokens_map.json',\n",
              " 'gpt2_adapter_casual/tokenizer.json/vocab.json',\n",
              " 'gpt2_adapter_casual/tokenizer.json/merges.txt',\n",
              " 'gpt2_adapter_casual/tokenizer.json/added_tokens.json',\n",
              " 'gpt2_adapter_casual/tokenizer.json/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, GPT2DoubleHeadsModel\n",
        "\n",
        "model = GPT2DoubleHeadsModel.from_pretrained('gpt2')\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "original_vocab_size = len(tokenizer)\n",
        "new_word_list = [i.strip().lower() for i in open(\"words.txt\").readlines()]\n",
        "num_new_words = len(new_word_list)\n",
        "num_added = tokenizer.add_tokens(new_word_list)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "#Compute the distribution from which we’ll sample:\n",
        "params = model.state_dict()\n",
        "embeddings = params['transformer.wte.weight']\n",
        "pre_expansion_embeddings = embeddings[:-num_new_words,:]\n",
        "mu = torch.mean(pre_expansion_embeddings, dim=0)\n",
        "n = pre_expansion_embeddings.size()[0]\n",
        "sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n\n",
        "dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
        "        mu, covariance_matrix=1e-5*sigma)\n",
        "\n",
        "#load in our new embeddings into the model:\n",
        "new_embeddings = torch.stack(tuple((dist.sample() for _ in range(num_new_words))), dim=0)\n",
        "embeddings[-num_new_words:,:] = new_embeddings\n",
        "params['transformer.wte.weight'][-num_new_words:,:] = new_embeddings\n",
        "model.load_state_dict(params)\n",
        "\n",
        "adapter_name = model.load_adapter(\"gpt2_adapter_mlm_model/adapter\")\n",
        "model.set_active_adapters(adapter_name)\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98MWJ2keej-n",
        "outputId": "8d586432-10ea-4cda-da94-6dd86e06f784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2DoubleHeadsModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['multiple_choice_head.summary.weight', 'multiple_choice_head.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "num_added_tokens = tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})\n",
        "# Update the model embeddings with the new vocabulary size\n",
        "# print(\"tokenizer length\", len(tokenizer))\n",
        "embedding_layer = model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "num_choices_arr = [5]\n",
        "model_ans = []\n",
        "for num_choices in num_choices_arr:\n",
        "\n",
        "  with open(f\"quiz_questions_{num_choices}.txt\") as f:\n",
        "    quiz_questions = json.load(f)\n",
        "  print(\"NUM_CHOICES:\", num_choices)\n",
        "\n",
        " \n",
        "\n",
        "  questions = quiz_questions\n",
        "  # answers = [0 for i in range(0, questions)]\n",
        "\n",
        "  counts = [0.0 for i in range(0, num_choices)]\n",
        "  total = 0.0\n",
        "  # print(\"question\", len(questions))\n",
        "  model.eval()\n",
        "  with torch.no_grad():        \n",
        "    for i, choices in enumerate(questions):\n",
        "        #print(choices)\n",
        "        # print(i)\n",
        "        if i % 1000 == 0 and i != 0:\n",
        "          print(f\"On iteration {i}, accuracy is {counts[0]/total} equal would be {1/num_choices} count is {total}\", [i/total for i in counts])\n",
        "        encoded_choices = [tokenizer.encode(s) for s in choices]\n",
        "        cls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]\n",
        "        #check if some choices are longer than the others\n",
        "        max_len = max([len(tokens) for tokens in encoded_choices])\n",
        "        check = True\n",
        "        for tokens in encoded_choices:\n",
        "            if len(tokens) < max_len:\n",
        "                check = False\n",
        "                break\n",
        "        if check:\n",
        "            input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\n",
        "            if len(input_ids) >= 1024:\n",
        "              continue\n",
        "            mc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1\n",
        "            input_ids = input_ids.to(device)\n",
        "            mc_token_ids = mc_token_ids.to(device)\n",
        "            outputs = model(input_ids, mc_token_ids=mc_token_ids)\n",
        "            lm_logits = outputs.logits\n",
        "            mc_logits = outputs.mc_logits\n",
        "            counts[mc_logits.argmax().item()] += 1\n",
        "            total += 1\n",
        "            model_ans.append(mc_logits.argmax().item())\n",
        "        else:\n",
        "          model_ans.append(-1)\n",
        "        torch.cuda.empty_cache() \n",
        "  print(f\"FINAL for num_choices {num_choices}: accuracy is {counts[0]/total} equal would be {1/num_choices} count is {total}\", [i/total for i in counts])\n",
        "\n",
        "  # print(counts)\n",
        "  # print(total)"
      ],
      "metadata": {
        "id": "M6ghBZ8qQThA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67ca0c8-79e4-4174-c4f1-9d0f71940e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Assigning [CLS] to the cls_token key of the tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NUM_CHOICES: 5\n",
            "On iteration 1000, accuracy is 0.18756268806419257 equal would be 0.2 count is 997.0 [0.18756268806419257, 0.20060180541624875, 0.18756268806419257, 0.21865596790371114, 0.20561685055165496]\n",
            "On iteration 2000, accuracy is 0.19808467741935484 equal would be 0.2 count is 1984.0 [0.19808467741935484, 0.19556451612903225, 0.19102822580645162, 0.21421370967741934, 0.20110887096774194]\n",
            "On iteration 3000, accuracy is 0.19912693082605776 equal would be 0.2 count is 2978.0 [0.19912693082605776, 0.19509738079247818, 0.19778374748153124, 0.20416386836803224, 0.2038280725319006]\n",
            "On iteration 4000, accuracy is 0.2047858942065491 equal would be 0.2 count is 3970.0 [0.2047858942065491, 0.19672544080604534, 0.19269521410579346, 0.2035264483627204, 0.20226700251889168]\n",
            "On iteration 5000, accuracy is 0.20757758968158002 equal would be 0.2 count is 4962.0 [0.20757758968158002, 0.19447803305118905, 0.198105602579605, 0.198105602579605, 0.20173317210802097]\n",
            "On iteration 6000, accuracy is 0.2098993288590604 equal would be 0.2 count is 5960.0 [0.2098993288590604, 0.1941275167785235, 0.19580536912751678, 0.1966442953020134, 0.2035234899328859]\n",
            "On iteration 7000, accuracy is 0.21362852213916045 equal would be 0.2 count is 6956.0 [0.21362852213916045, 0.191345600920069, 0.1973835537665325, 0.19493962047153537, 0.20270270270270271]\n",
            "On iteration 8000, accuracy is 0.2134223117602619 equal would be 0.2 count is 7942.0 [0.2134223117602619, 0.19113573407202217, 0.19780911609166457, 0.1944094686476958, 0.2032233694283556]\n",
            "On iteration 9000, accuracy is 0.20987239758226997 equal would be 0.2 count is 8934.0 [0.20987239758226997, 0.19241101410342512, 0.19778374748153124, 0.19610476830087306, 0.2038280725319006]\n",
            "On iteration 10000, accuracy is 0.20931170009069838 equal would be 0.2 count is 9923.0 [0.20931170009069838, 0.1933890960395042, 0.19953643051496522, 0.19621082333971582, 0.2015519500151164]\n",
            "On iteration 11000, accuracy is 0.20897846999541914 equal would be 0.2 count is 10915.0 [0.20897846999541914, 0.19431974347228584, 0.19770957398076042, 0.1971598717361429, 0.20183234081539167]\n",
            "On iteration 12000, accuracy is 0.20922224088694777 equal would be 0.2 count is 11906.0 [0.20922224088694777, 0.19502771711741979, 0.19922728036284226, 0.19637157735595498, 0.2001511842768352]\n",
            "FINAL for num_choices 5: accuracy is 0.2085843995047462 equal would be 0.2 count is 12115.0 [0.2085843995047462, 0.19496491952125464, 0.19958728848534874, 0.19636813867106892, 0.20049525381758151]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"compare_5/adapter.txt\", \"w+\")\n",
        "for i in model_ans:\n",
        "  f.write(str(i)+\"\\n\")\n",
        "f.close()\n",
        "print(len(model_ans))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RyP3_X7iS_O",
        "outputId": "ff5881f8-92b0-490c-a9a1-e47170b1fb97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls compare_5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XOZ_IIBktEI",
        "outputId": "2a901ead-f595-4f0b-deae-99beaa3d3f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adapter.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 3):\n",
        "  print(quiz_questions[i])\n",
        "  print(quiz_answers[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zjb5cbNpR84T",
        "outputId": "614e3b91-118a-46af-d66c-272a9d2acaca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['that kid is such a cuffy [CLS]', 'that kid is such a cuffy [CLS]', 'that kid is such a cuffy [CLS]', 'that kid is such a cuffy [CLS]', 'that kid is such a cuffy [CLS]']\n",
            "0\n",
            "[\"Let's go spooch one down brah [CLS]\", \"Let's go nussy one down brah [CLS]\", \"Let's go Noiners one down brah [CLS]\", \"Let's go dizzle one down brah [CLS]\", \"Let's go civic one down brah [CLS]\"]\n",
            "0\n",
            "['Dude, did that (American Muscle/Opel/just about any performance vehicle) just completely obliterate that WRX? [CLS]', 'Dude, did that (American Muscle/Opel/just about any performance vehicle) just completely obliterate that WRX? [CLS]', 'Dude, did that (American Muscle/Opel/just about any performance vehicle) just completely obliterate that WRX? [CLS]', 'Dude, did that (American Muscle/Opel/just about any performance vehicle) just completely obliterate that WRX? [CLS]', 'Dude, did that (American Muscle/Opel/just about any performance vehicle) just completely obliterate that WRX? [CLS]']\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a [CLS] to the vocabulary (we should train it also!)\n",
        "num_added_tokens = tokenizer.add_special_tokens({\"cls_token\": \"[CLS]\"})\n",
        "# Update the model embeddings with the new vocabulary size\n",
        "print(len(tokenizer))\n",
        "embedding_layer = model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "questions = quiz_questions\n",
        "answers = quiz_answers\n",
        "\n",
        "count0 = 0.0\n",
        "count1 = 0.0\n",
        "count2 = 0.0\n",
        "count3 = 0.0\n",
        "count4 = 0.0\n",
        "total = 0.0\n",
        "print(\"question\", len(questions))\n",
        "model.eval()\n",
        "with torch.no_grad():        \n",
        "  for i, choices in enumerate(questions):\n",
        "      #print(choices)\n",
        "      # print(i)\n",
        "      if i % 100 == 0:\n",
        "        print(i)\n",
        "      encoded_choices = [tokenizer.encode(s) for s in choices]\n",
        "      cls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]\n",
        "      #check if some choices are longer than the others\n",
        "      max_len = max([len(tokens) for tokens in encoded_choices])\n",
        "      check = True\n",
        "      for tokens in encoded_choices:\n",
        "          if len(tokens) < max_len:\n",
        "              check = False\n",
        "      if check:\n",
        "          input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\n",
        "          if len(input_ids) >= 1024:\n",
        "            continue\n",
        "          mc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1\n",
        "          input_ids = input_ids.to(device)\n",
        "          mc_token_ids = mc_token_ids.to(device)\n",
        "          outputs = model(input_ids, mc_token_ids=mc_token_ids)\n",
        "          lm_logits = outputs.logits\n",
        "          mc_logits = outputs.mc_logits\n",
        "          if mc_logits.argmax().item() == 0:\n",
        "              count0 += 1\n",
        "          elif mc_logits.argmax().item() == 1:\n",
        "              count1 += 1\n",
        "          elif mc_logits.argmax().item() == 2:\n",
        "              count2 += 1\n",
        "          elif mc_logits.argmax().item() == 3:\n",
        "              count3 += 1\n",
        "          elif mc_logits.argmax().item() == 4:\n",
        "              count4 += 1\n",
        "          \n",
        "          total += 1\n",
        "\n",
        "print(len(questions))\n",
        "print(count0)\n",
        "print(count1)\n",
        "print(count2)\n",
        "print(count3)\n",
        "print(count4)\n",
        "print(total)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIEmPtAiQNEu",
        "outputId": "20e25e27-a4e0-4a56-8805-a9ca552565f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Assigning [CLS] to the cls_token key of the tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79247\n",
            "question 4433\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4433\n",
            "815.0\n",
            "714.0\n",
            "688.0\n",
            "686.0\n",
            "727.0\n",
            "3630.0\n"
          ]
        }
      ]
    }
  ]
}